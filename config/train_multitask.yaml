model: mistralai/Voxtral-Mini-3B-2507
device_id: 0

data:
  train_manifest: /mnt/d/MEGAN/data/moofoo_s2tt/ms/testing_updated.json
  eval_manifest: /mnt/d/MEGAN/data/moofoo_s2tt/ms/testing_updated.json

lora:
  r: 8
  alpha: 32
  dropout: 0.0
  target_modules: ["q_proj", "k_proj"]

tasks:
  s2tt:
    enabled: True
    incl_src_lang: False
  asr:
    enabled: True
  t2t:
    enabled: True
  lid:  
    enabled: True

trainer:
  # whether or not to add task token routing e.g. <lid>, <asr>, <t2t>, <s2tt>
  use_task_routing: True

  ### Loss manipulation
  lang_class_weighting: False
  task_uncertainty_weighting: False

  ### Gradient manipulation
  # Note: pcgrad cannot be used with DDP
  use_pcgrad: False
  pcgrad_reduction: "mean"

  epochs: 3
  lr: 5e-5
  grad_accum: 4
  train_batch_size: 2
  eval_batch_size: 2
  logging_steps: 2
  save_steps: 10
  eval_steps: 10
  warmup_steps: 500
  eval_on_start: False
  load_in_4bit: False
  bf16: True
  save_total_limit: 2
  resume_from_checkpoint: null # ckpt folder or null

exp_manager:
  name: TEST_MULTITASK_T2T
  exp_dir: experiments
  logger: tensorboard # wandb or tensorboard
  wandb:
    project: voxtral-finetuning-multitask
    run_id: null # only used for resuming runs
  