model: mistralai/Voxtral-Mini-3B-2507

data:
  train_manifest: /mnt/d/MEGAN/data/PORT/mms_set_1/dev_split/dev_manifest_short.json
  eval_manifest: /mnt/d/MEGAN/data/PORT/mms_set_1/dev_split/dev_manifest_short.json

lora:
  r: 8
  alpha: 32
  dropout: 0.0
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

trainer:
  epochs: 3
  lr: 5e-5
  grad_accum: 4
  train_batch_size: 2
  eval_batch_size: 4
  logging_steps: 2
  save_steps: 250
  eval_steps: 250
  warmup_ratio: 0.05
  bf16: False
  save_total_limit: 2
  resume_from_checkpoint: null # ckpt folder or null

exp_manager:
  name: testtest
  exp_dir: experiments
  logger: wandb # wandb or tensorboard
  wandb:
    project: voxtral-lora-finetuning
    run_id: null # only used for resuming runs
  